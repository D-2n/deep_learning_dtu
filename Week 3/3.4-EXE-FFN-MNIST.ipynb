{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "> This code is a slight modification to a translation (TensorFlow --> PyTorch) of a previous version of the [02456](http://kurser.dtu.dk/course/02456) course material. \n",
    "> [Original repo link (TensorFlow)](https://github.com/DeepLearningDTU/02456-deep-learning).\n",
    "> [Translated repo link (PyTorch)](https://github.com/munkai/pytorch-tutorial/tree/master/2_intermediate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset\n",
    "MNIST is a dataset that is often used for benchmarking. The MNIST dataset consists of 70,000 images of handwritten digits from 0-9. The dataset is split into a 50,000 images training set, 10,000 images validation set and 10,000 images test set. The images are 28x28 pixels, where each pixel represents a normalised value between 0-255 (0=black and 255=white).\n",
    "\n",
    "![MNIST.Exampel](../static_files/mnist.png)\n",
    "\n",
    "\n",
    "## Primer\n",
    "We use a feedforward neural network to classify the 28x28 mnist images. `num_features` is therefore $28 * 28=784$, i.e. we represent each image as a vector. The ordering of the pixels in the vector does not matter, so we could permutate all images using the same permutation and still get the same performance. (You are of course encouraged to try this using ``numpy.random.permutation`` to get a random permutation. This task is therefore called the _permutation invariant_ MNIST. Obviously this throws away a lot of structure in the data. In the next module we'll fix this with the convolutional neural network wich encodes prior knowledgde about data that has either spatial or temporal structure.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MNIST\n",
    "First let's load the MNIST dataset and plot a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = MNIST(\"./temp/\", train=True, download=True)\n",
    "mnist_testset = MNIST(\"./temp/\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed up training we'll only work on a subset of the data\n",
    "x_train = mnist_trainset.data[:1000].view(-1, 784).float()\n",
    "targets_train = mnist_trainset.targets[:1000]\n",
    "\n",
    "x_valid = mnist_trainset.data[1000:1500].view(-1, 784).float()\n",
    "targets_valid = mnist_trainset.targets[1000:1500]\n",
    "\n",
    "x_test = mnist_testset.data[:500].view(-1, 784).float()\n",
    "targets_test = mnist_testset.targets[:500]\n",
    "\n",
    "print(\"Information on dataset\")\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"targets_train\", targets_train.shape)\n",
    "print(\"x_valid\", x_valid.shape)\n",
    "print(\"targets_valid\", targets_valid.shape)\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"targets_test\", targets_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot a few MNIST examples\n",
    "idx, dim, classes = 0, 28, 10\n",
    "# create empty canvas\n",
    "canvas = np.zeros((dim*classes, classes*dim))\n",
    "\n",
    "# fill with tensors\n",
    "for i in range(classes):\n",
    "    for j in range(classes):\n",
    "        canvas[i*dim:(i+1)*dim, j*dim:(j+1)*dim] = x_train[idx].reshape((dim, dim))\n",
    "        idx += 1\n",
    "\n",
    "# visualize matrix of tensors as gray scale image\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.axis('off')\n",
    "plt.imshow(canvas, cmap='gray')\n",
    "plt.title('MNIST handwritten digits')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "One of the large challenges in deep learning is the amount of hyperparameters that needs to be selected, and the lack of a good principled way of selecting them.\n",
    "Hyperparameters can be found by experience (guessing) or some search procedure (often quite slow).\n",
    "Random search is easy to implement and performs decent: http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf . \n",
    "More advanced search procedures include [Spearmint](https://github.com/JasperSnoek/spearmint) and many others.\n",
    "\n",
    "**In practice a lot of trial and error is almost always involved.** This can be frustrating and time consuming, but the best thing to do is to think as a scientist, and go about it in a ordered manner --> monitor as much as you can, take notes, and be deliberate!\n",
    "\n",
    "Below are some guidelines that you can use as a starting point to some of the most important hyperparameters. \n",
    "(*regularization* is also very important, but will be covered later.)\n",
    "\n",
    "\n",
    "### Ballpark estimates of hyperparameters\n",
    "__Number of hidden units and network structure:__\n",
    "You'll have to experiment. One rarely goes below 512 units for feedforward networks (unless your are training on CPU...).\n",
    "There's some research into stochastic depth networks: https://arxiv.org/pdf/1603.09382v2.pdf, but in general this is trial and error.\n",
    "\n",
    "__Parameter initialization:__\n",
    "Parameter initialization is extremely important.\n",
    "PyTorch has a lot of different initializers, check the [PyTorch API](http://pytorch.org/docs/master/nn.html#torch-nn-init). Often used initializer are\n",
    "1. Kaiming He\n",
    "2. Xavier Glorot\n",
    "3. Uniform or Normal with small scale (0.1 - 0.01)\n",
    "4. Orthogonal (this usually works very well for RNNs)\n",
    "\n",
    "Bias is nearly always initialized to zero using the [torch.nn.init.constant(tensor, val)](http://pytorch.org/docs/master/nn.html#torch.nn.init.constant)\n",
    "\n",
    "__Mini-batch size:__\n",
    "Usually people use 16-256. Bigger is not allways better. With smaller mini-batch size you get more updates and your model might converge faster. Also small batch sizes use less memory, which means you can train a model with more parameters.\n",
    "\n",
    "__Nonlinearity:__ [The most commonly used nonliearities are](http://pytorch.org/docs/master/nn.html#non-linear-activations)\n",
    "1. ReLU\n",
    "2. Leaky ReLU\n",
    "3. ELU\n",
    "3. Sigmoid (rarely, if ever, used in hidden layers anymore, squashes the output to the interval [0, 1] - appropriate if the targets are binary.\n",
    "4. Tanh is similar to the sigmoid, but squashes to [-1, 1]. Rarely used any more.\n",
    "4. Softmax normalizes the output to 1, usrful if you have a multi-class classification problem.\n",
    "\n",
    "See the plot below.\n",
    "\n",
    "__Optimizer and learning rate:__\n",
    "1. SGD + Momentum: learning rate 0.01 - 0.1 \n",
    "2. ADAM: learning rate 3e-4 - 1e-5\n",
    "3. RMSPROP: somewhere between SGD and ADAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate different output units\n",
    "x = np.linspace(-6, 6, 100)\n",
    "units = {\n",
    "    \"ReLU\": lambda x: np.maximum(0, x),\n",
    "    \"Leaky ReLU\": lambda x: np.maximum(0, x) + 0.1 * np.minimum(0, x),\n",
    "    \"Elu\": lambda x: (x > 0) * x + (1 - (x > 0)) * (np.exp(x) - 1),\n",
    "    \"Sigmoid\": lambda x: (1 + np.exp(-x))**(-1),\n",
    "    \"tanh\": lambda x: (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "[plt.plot(x, unit(x), label=unit_name, lw=2) for unit_name, unit in units.items()]\n",
    "plt.legend(loc=2, fontsize=16)\n",
    "plt.title('Non-linearities', fontsize=20)\n",
    "plt.ylim([-2, 5])\n",
    "plt.xlim([-6, 6])\n",
    "\n",
    "# assert that all class probablities sum to one\n",
    "softmax = lambda x: np.exp(x) / np.sum(np.exp(x))\n",
    "print(\"softmax should sum to one (approxiamtely):\", np.sum(softmax(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "num_classes = 10\n",
    "num_l1 = 512\n",
    "num_features = x_train.shape[1]\n",
    "\n",
    "# define network\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_hidden, num_output):\n",
    "        super(Net, self).__init__()  \n",
    "        # input layer\n",
    "        self.W_1 = Parameter(init.xavier_normal_(torch.Tensor(num_hidden, num_features)))\n",
    "        self.b_1 = Parameter(init.constant_(torch.Tensor(num_hidden), 0))\n",
    "        # hidden layer\n",
    "        self.W_2 = Parameter(init.xavier_normal_(torch.Tensor(num_output, num_hidden)))\n",
    "        self.b_2 = Parameter(init.constant_(torch.Tensor(num_output), 0))\n",
    "        # define activation function in constructor\n",
    "        self.activation = torch.nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.linear(x, self.W_1, self.b_1)\n",
    "        x = self.activation(x)\n",
    "        x = F.linear(x, self.W_2, self.b_2)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(num_features, num_l1, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the forward pass with dummy data\n",
    "x = np.random.normal(0, 1, (45, dim*dim)).astype('float32')\n",
    "\n",
    "print(net(torch.from_numpy(x)).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the training loop\n",
    "\n",
    "We train the network by calculating the gradient w.r.t the cost function and update the parameters in direction of the negative gradient. \n",
    "\n",
    "\n",
    "When training neural network you always use mini batches. Instead of calculating the average gradient using the entire dataset you approximate the gradient using a mini-batch of typically 16 to 256 samples. The paramters are updated after each mini batch. Networks converge much faster using mini batches because the parameters are updated more often.\n",
    "\n",
    "We build a loop that iterates over the training data. Remember that the parameters are updated each time ``optimizer.step()`` is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could have done this ourselves,\n",
    "# but we should be aware of sklearn and its tools\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# normalize the inputs\n",
    "x_train.div_(255)\n",
    "x_valid.div_(255)\n",
    "x_test.div_(255)\n",
    "\n",
    "# setting hyperparameters and gettings epoch sizes\n",
    "batch_size = 100\n",
    "num_epochs = 200\n",
    "num_samples_train = x_train.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = x_valid.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "losses = []\n",
    "\n",
    "get_slice = lambda i, size: range(i * size, (i + 1) * size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    ## Train\n",
    "    cur_loss = 0\n",
    "    net.train()\n",
    "    for i in range(num_batches_train):\n",
    "        optimizer.zero_grad()\n",
    "        slce = get_slice(i, batch_size)\n",
    "        output = net(x_train[slce])\n",
    "        \n",
    "        # compute gradients given loss\n",
    "        target_batch = targets_train[slce]\n",
    "        batch_loss = criterion(output, target_batch)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cur_loss += batch_loss   \n",
    "    losses.append(cur_loss / batch_size)\n",
    "\n",
    "    net.eval()\n",
    "    ### Evaluate training\n",
    "    train_preds, train_targs = [], []\n",
    "    for i in range(num_batches_train):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        output = net(x_train[slce])\n",
    "        \n",
    "        preds = torch.max(output, 1)[1]\n",
    "        \n",
    "        train_targs += list(targets_train[slce].numpy())\n",
    "        train_preds += list(preds.data.numpy())\n",
    "    \n",
    "    ### Evaluate validation\n",
    "    val_preds, val_targs = [], []\n",
    "    for i in range(num_batches_valid):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        \n",
    "        output = net(x_valid[slce])\n",
    "        preds = torch.max(output, 1)[1]\n",
    "        val_targs += list(targets_valid[slce].numpy())\n",
    "        val_preds += list(preds.data.numpy())\n",
    "        \n",
    "\n",
    "    train_acc_cur = accuracy_score(train_targs, train_preds)\n",
    "    valid_acc_cur = accuracy_score(val_targs, val_preds)\n",
    "    \n",
    "    train_acc.append(train_acc_cur)\n",
    "    valid_acc.append(valid_acc_cur)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (\n",
    "                epoch+1, losses[-1], train_acc_cur, valid_acc_cur))\n",
    "\n",
    "epoch = np.arange(len(train_acc))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')\n",
    "plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignments\n",
    "\n",
    "Try and add these modifications (might require some Googleing -- an important skill in deep learning):\n",
    "- Kaiming He initialization instead of Xavier Glorot\n",
    "- add an extra layer\n",
    "- use the relu activation function\n",
    "- add momentum to the optimizer\n",
    "- use the ADAM optimizer instead of stochastic gradient descent\n",
    "\n",
    "### Advanced - Regularization\n",
    "\n",
    "Regularization is VERY important in practice and is used practically every time.\n",
    "Many important results are completely dependent on clever use of regularization, and it is something you need to become familiar with if you want to work with deep learning.\n",
    "\n",
    "- add L1 or L2 weight regularization (aka. weight decay) \n",
    "- add dropout to the network (**note** the `net.train()` and `net.eval()` are already in the code)\n",
    "- add batchnorm\n",
    "\n",
    "__Pointers on regularization hyperparameter:__\n",
    "1. L2 and [L1 regularization](http://pytorch.org/docs/master/nn.html#torch.nn.L1Loss) (weight decay of optimization functions) \n",
    "  - Normal ranges: 1e-4  -  1e-8\n",
    "1. [Dropout](http://pytorch.org/docs/master/nn.html?highlight=dropout#torch.nn.Dropout). Dropout rate 0.1-0.5\n",
    "  - Remember to pick the correct version according to the input dimensionality\n",
    "  - **NOTE** call `net.train()` before training to activate random dropout, and call `net.eval()` to deactivate dropout while validating or running inference with model.\n",
    "1. [Batchnorm](http://pytorch.org/docs/master/nn.html#torch.nn.BatchNorm1d): Batchnorm also acts as a regularizer - Often very useful (faster and better convergence)\n",
    "  - Remember to pick the correct version according to the input dimensionality\n",
    "  - **NOTE** call `net.train()` before training to activate, and call `net.eval()` to have a non-stochastic variant while validating or running inference with model.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are done for now. [Good job.](https://media1.tenor.com/images/0fd559b07f2174f9b8b7dbde7c5a67ca/tenor.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
